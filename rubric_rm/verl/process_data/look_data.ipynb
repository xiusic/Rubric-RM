{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601e04a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiusic/anaconda3/envs/RubricRM/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b4b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"gaotang/filtered_sky_code_8k_math_10k_rubric_evidence_classify_weight\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4cce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI Chatbots to the Client's question displayed below.\n",
      "\n",
      "First, classify the task into one of two categories: <type>Reasoning</type> or <type>Chat</type>.\n",
      "- Use <type>Reasoning</type> for tasks that involve math, coding, or require domain knowledge, multi-step inference, logical deduction, or combining information to reach a conclusion.\n",
      "- Use <type>Chat</type> for tasks that involve open-ended or factual conversation, stylistic rewrites, safety questions, or general helpfulness requests without deep reasoning.\n",
      "\n",
      "If the task is Reasoning:\n",
      "1. Solve the Client's question yourself and present your final answer within <solution>...</solution> tags.\n",
      "2. Evaluate the two Chatbot responses based on correctness, completeness, and reasoning quality, referencing your own solution.\n",
      "3. Include your evaluation inside <eval>...</eval> tags, quoting or summarizing the Chatbots using the following tags:\n",
      "   - <quote_A>...</quote_A> for direct quotes from Chatbot A\n",
      "   - <summary_A>...</summary_A> for paraphrases of Chatbot A\n",
      "   - <quote_B>...</quote_B> for direct quotes from Chatbot B\n",
      "   - <summary_B>...</summary_B> for paraphrases of Chatbot B\n",
      "4. End with your final judgment in the format: <answer>[[A]]</answer> or <answer>[[B]]</answer>\n",
      "\n",
      "If the task is Chat:\n",
      "1. Generate evaluation criteria (rubric) tailored to the Client's question and context, enclosed in <rubric>...</rubric> tags.\n",
      "2. Assign weights to each rubric item based on their relative importance.\n",
      "3. Inside <rubric>, include a <justify>...</justify> section explaining why you chose those rubric criteria and weights.\n",
      "4. Compare both Chatbot responses according to the rubric.\n",
      "5. Provide your evaluation inside <eval>...</eval> tags, using <quote_A>, <summary_A>, <quote_B>, and <summary_B> as described above.\n",
      "6. End with your final judgment in the format: <answer>[[A]]</answer> or <answer>[[B]]</answer>\n",
      "\n",
      "Important Notes:\n",
      "- Be objective and base your evaluation only on the content of the responses.\n",
      "- Do not let response order, length, or Chatbot names affect your judgment.\n",
      "- Follow the response format strictly depending on the task type.\n",
      "\n",
      "Your output must follow one of the two formats below:\n",
      "\n",
      "For Reasoning:\n",
      "<type>Reasoning</type>\n",
      "\n",
      "<solution> your own solution for the problem </solution>\n",
      "\n",
      "<eval>\n",
      "  include direct comparisons supported by <quote_A>...</quote_A> or <summary_A>...</summary_A>, and <quote_B>...</quote_B>, or <summary_B>...</summary_B>\n",
      "</eval>\n",
      "\n",
      "<answer>[[A/B]]</answer>\n",
      "\n",
      "For Chat:\n",
      "<type>Chat</type>\n",
      "\n",
      "<rubric>\n",
      "  detailed rubric items\n",
      "  <justify> justification for the rubric </justify>\n",
      "</rubric>\n",
      "\n",
      "<eval>\n",
      "  include direct comparisons supported by <quote_A>...</quote_A> or <summary_A>...</summary_A>, and <quote_B>...</quote_B>, or <summary_B>...</summary_B> tags\n",
      "</eval>\n",
      "\n",
      "<answer>[[A/B]]</answer>\n"
     ]
    }
   ],
   "source": [
    "print(ds[0]['context_messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d4a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RubricRM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
